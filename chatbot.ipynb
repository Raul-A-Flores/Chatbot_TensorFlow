{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a Chatbot Using Python, NLTK, and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell so you do not see GPU availibility errors from tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Import the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')\n",
    "with open(r'intents.json') as data:\n",
    "    intents = json.loads(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        classes.append(intent['tag'])\n",
    "        documents.append((w,intent['tag']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "ignore_words= ['?', '!']\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes,open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Create Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) \n",
    "    for word in pattern_words]\n",
    "    for w in words:\n",
    "        if w in pattern_words:\n",
    "            bag.append(1)\n",
    "        else: bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "\n",
    "random.shuffle(training)\n",
    "a = int(0.7*len(training))\n",
    "training = np.array(training, dtype = 'object')\n",
    "X_train = list(training[:a, 0])\n",
    "y_train = list(training[:a, 1])\n",
    "X_val = list(training[a:, 0])\n",
    "y_val = list(training[a:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Design the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(y_train[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(np.array(X_train), np.array(y_train), epochs=200, batch_size=5, validation_data=(X_val, y_val), verbose=1)\n",
    "model.save('trained_model.h5', hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Print the training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "N = np.arange(0,200)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, hist.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, hist.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(N, hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
